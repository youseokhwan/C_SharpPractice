# 신경망의 수학적 요소

### MNIST
* 딥러닝 계의 Hello World
* 오래된 역사와 함께 많은 연구에 사용됨
* NumPy 배열 형태로 Keras에 이미 내장됨
* 6만 개의 train_image, train_label와 1만 개의 test_image, test_label로 구성

### ㅁㄴㅇㄹ
* 학습 전에 데이터를 네트워크에 맞는 크기로 바꾸고, 0과 1 사이로 스케일 조정
* test set는 train set보다 정확도가 낮음 (과대적합(overfitting))

### 텐서(tensor)
* 데이터 저장을 위한 컨테이너
* 스칼라(0D): 하나의 숫자
* 벡터(1D): 스칼라 배열
* 행렬(2D): 벡터 배열
* ... 동영상의 경우 5D까지 다루기도 함
* 핵심속성 3가지로 정의: 축의 개수(랭크), 크기(shape), 데이터 타입

### 슬라이싱
* [10:100, 0:28, 0:28] -> (90, 28, 28)
* [10:100, :, :] -> 동일
* [:, 7:-7, 7:-7] -> 음수는 상대적인 위치를 의미, 뒤에서 7번째까지 선택하겠다는 의미

### 텐서 연산
* 이진수의 이항 연산처럼 신경망에는 텐서 연산이 존재
* 원소별 연산(element-wise operation)
* 브로드캐스팅: 작은 텐서가 큰 텐서에 맞추어 확장하여 연산
* 점곱 연산(dot operation): 
* 텐서 크기 변환(tensor reshaping): 특정 크기에 맞게 행과 열을 재배열(원소 개수 동일함)

### 그래디언트
* 신경망에선 모두 미분 가능한 함수(연속됨, 기울기가 갑자기 바뀌지 않음)
* gradient: 텐서 연산의 변화율(기울기들의 텐서?)
* 미분 가능한 함수들이 주어지면 이론적으로 이 함수의 최솟값을 구할 수 있음(변화율이 0인 지점)
* 우리는 변화율이 0이 되는 지점을 모두 찾고 어떤 값이 가장 작은지 확인해야 함(가장 작은 loss function의 값을 찾는 것)
* 미니 배치 확률적 경사 하강법(mini-batch stochastic gradient descent, mini-batch SGD)
~~~
1. 훈련 샘플 배치 x와 그에 상응하는 y 추출
2. x로 네트워크 실행하고 예측 y_pred를 구함
3. y와 y_pred의 오차 측정하여 손실 계산
4. 손실 함수의 gradient 계산(역방향 패스)
5. gradient의 반대 방향으로 이동
~~~
* step값 적절히 골라야 함(값이 너무 작으면 너무 많은 반복, 지역 최솟값에 갇힐 수 있음, 너무 크면 완전히 다른 위치로 이동할 수도 있음)
* #사람에게 1000000차원을 이해시킬 수 없어서 저차원으로 이 모델을 보게되는데, 이러면 지역 최솟값에 자주 갇히지 않을까 하는 생각이 들 수 있음
* #그러나 고차원에서 지역 최솟값에 갇히는 경우는 생각보다 매우 드물다.

### 옵티마이저
* SGD 변종: 모멘텀 사용 SGD, Adagrad, RMSProp 등 -> 옵티마이저
* 다음 weight를 계산할 때 현재 gradient만 보는 것이 아닌 이전 weight와 함께 여러 다른 방식으로 고려함
* 특히 모멘텀(momentum)은 SGD의 2개 문제점인 수렴 속도와 지역 최솟값을 해결함

### Backpropagation
* 최종 손실 값에서부터 시작하여 연쇄적으로 하위층의 손실 값 계산?
